<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM for Table data enhancement (CN)">
<meta property="og:url" content="http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/index.html">
<meta property="og:site_name" content="Yuhang&#39;s AI Journey">
<meta property="og:description" content="© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-08-13T20:40:32.000Z">
<meta property="article:modified_time" content="2024-08-14T16:19:13.209Z">
<meta property="article:author" content="YuhangWu">
<meta property="article:tag" content="CN Blog">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/logo.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/logo.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.png">
        
      
    
    <!-- title -->
    <title>LLM for Table data enhancement (CN)</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/YuhangWuAI">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/08/14/Dataset-for-Question-Answering-CN/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/08/13/RAG-End2end-Frame-and-Vector-DB-Summary/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&text=LLM for Table data enhancement (CN)"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&is_video=false&description=LLM for Table data enhancement (CN)"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=LLM for Table data enhancement (CN)&body=Check out this article: http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&name=LLM for Table data enhancement (CN)&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&t=LLM for Table data enhancement (CN)"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-for-Table"><span class="toc-number">1.</span> <span class="toc-text">LLM for Table</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-LLMs%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.</span> <span class="toc-text">1. LLMs表格处理方法分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95%E5%BD%92%E7%BA%B3"><span class="toc-number">1.2.</span> <span class="toc-text">具体方法归纳</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-on-Tabular-Data-Retriever"><span class="toc-number">2.</span> <span class="toc-text">LLM on Tabular Data (Retriever)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E6%99%AE%E9%81%8D%E8%83%BD%E5%8A%9B"><span class="toc-number">2.1.</span> <span class="toc-text">5.2 大型语言模型在问答任务中的普遍能力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E9%97%AE%E7%AD%94%EF%BC%88Numerical-QA%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">数值问答（Numerical QA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Text2SQL"><span class="toc-number">2.3.</span> <span class="toc-text">Text2SQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%A7%E5%B0%8F%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.4.</span> <span class="toc-text">模型大小对性能的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E8%BF%98%E6%98%AF%E4%B8%8D%E5%BE%AE%E8%B0%83%EF%BC%9F"><span class="toc-number">2.5.</span> <span class="toc-text">微调还是不微调？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E8%A1%A8%E6%A0%BC%E9%97%AE%E7%AD%94%E7%9A%84%E5%85%B3%E9%94%AE%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="toc-number">2.6.</span> <span class="toc-text">5.3 表格问答的关键组成部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-1-%E6%9F%A5%E8%AF%A2%E6%84%8F%E5%9B%BE%E6%B6%88%E6%AD%A7"><span class="toc-number">2.7.</span> <span class="toc-text">5.3.1 查询意图消歧</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E5%92%8C%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95%E5%BD%92%E7%BA%B3%E6%80%BB%E7%BB%93"><span class="toc-number">2.8.</span> <span class="toc-text">搜索和检索方法归纳总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A3%80%E7%B4%A2%E6%A8%A1%E5%9D%97%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87"><span class="toc-number">2.9.</span> <span class="toc-text">1. 检索模块性能提升</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A1%A8%E6%A0%BC%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95"><span class="toc-number">2.10.</span> <span class="toc-text">2. 表格采样方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%A1%A8%E6%A0%BC%E6%8E%92%E5%90%8D%E6%96%B9%E6%B3%95"><span class="toc-number">2.11.</span> <span class="toc-text">3. 表格排名方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%B8%89%E6%AD%A5%E6%A3%80%E7%B4%A2%E6%9E%B6%E6%9E%84"><span class="toc-number">2.12.</span> <span class="toc-text">4. 三步检索架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E7%A8%A0%E5%AF%86%E5%92%8C%E7%A8%80%E7%96%8F%E6%A3%80%E7%B4%A2%E5%99%A8%E7%BB%93%E5%90%88"><span class="toc-number">2.13.</span> <span class="toc-text">5. 稠密和稀疏检索器结合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%8E%B7%E5%8F%96%E9%A2%9D%E5%A4%96%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">2.14.</span> <span class="toc-text">6. 获取额外信息的方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%89%8B%E5%8A%A8%E5%92%8C%E9%9A%8F%E6%9C%BA%E7%A4%BA%E4%BE%8B%E9%80%89%E6%8B%A9"><span class="toc-number">2.15.</span> <span class="toc-text">7. 手动和随机示例选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LI-RAGE"><span class="toc-number">3.</span> <span class="toc-text">LI-RAGE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E7%AB%A0%E5%86%85%E5%AE%B9%E7%AE%80%E7%9F%AD%E6%80%BB%E7%BB%93"><span class="toc-number">3.1.</span> <span class="toc-text">文章内容简短总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E9%97%AE%E9%A2%98"><span class="toc-number">3.2.</span> <span class="toc-text">示例问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.3.</span> <span class="toc-text">表格数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">3.4.</span> <span class="toc-text">全流程示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%A1%A8%E6%A0%BC%E6%A3%80%E7%B4%A2%EF%BC%88Retriever%EF%BC%89"><span class="toc-number">3.4.1.</span> <span class="toc-text">1. 表格检索（Retriever）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%EF%BC%88Reader%EF%BC%89"><span class="toc-number">3.4.2.</span> <span class="toc-text">2. 答案生成（Reader）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BA%8C%E8%BF%9B%E5%88%B6%E7%9B%B8%E5%85%B3%E6%80%A7%E6%A0%87%E8%AE%B0%EF%BC%88Binary-Relevance-Token%EF%BC%89"><span class="toc-number">3.4.3.</span> <span class="toc-text">3. 二进制相关性标记（Binary Relevance Token）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E8%BF%87%E6%BB%A4%E4%B8%8E%E7%A1%AE%E5%AE%9A%E6%9C%80%E7%BB%88%E7%AD%94%E6%A1%88"><span class="toc-number">3.4.4.</span> <span class="toc-text">4. 过滤与确定最终答案</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TAP4LLM"><span class="toc-number">4.</span> <span class="toc-text">TAP4LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TAP4LLM%E6%96%B9%E6%B3%95%E8%AF%A6%E7%BB%86%E6%80%BB%E7%BB%93"><span class="toc-number">4.1.</span> <span class="toc-text">TAP4LLM方法详细总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">4.2.</span> <span class="toc-text">大型语言模型在表格数据中的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF"><span class="toc-number">4.3.</span> <span class="toc-text">表格增强技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TAP4LLM%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">4.4.</span> <span class="toc-text">TAP4LLM的核心组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">4.5.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%90%E5%88%B6"><span class="toc-number">4.6.</span> <span class="toc-text">限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E4%BD%BF%E7%94%A8TAP4LLM%E8%BF%9B%E8%A1%8C%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">4.7.</span> <span class="toc-text">示例：使用TAP4LLM进行表格数据分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%A1%A8%E6%A0%BC%E9%87%87%E6%A0%B7"><span class="toc-number">4.8.</span> <span class="toc-text">1. 表格采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A1%A8%E6%A0%BC%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.9.</span> <span class="toc-text">2. 表格增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%A1%A8%E6%A0%BC%E6%89%93%E5%8C%85%E4%B8%8E%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">4.10.</span> <span class="toc-text">3. 表格打包与序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%9C%80%E7%BB%88%E5%88%86%E6%9E%90%E7%BB%93%E6%9E%9C"><span class="toc-number">4.11.</span> <span class="toc-text">4. 最终分析结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90%EF%BC%9A%E4%BD%BF%E7%94%A8TAP4LLM%E5%9B%9E%E7%AD%94%E9%87%91%E8%9E%8D%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98"><span class="toc-number">4.12.</span> <span class="toc-text">例子：使用TAP4LLM回答金融数据问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E8%A1%A8%E6%A0%BC%E5%92%8C%E9%97%AE%E9%A2%98"><span class="toc-number">4.13.</span> <span class="toc-text">1. 输入表格和问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A1%A8%E6%A0%BC%E9%87%87%E6%A0%B7"><span class="toc-number">4.14.</span> <span class="toc-text">2. 表格采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%A1%A8%E6%A0%BC%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.15.</span> <span class="toc-text">3. 表格增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%A1%A8%E6%A0%BC%E6%89%93%E5%8C%85%E4%B8%8E%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">4.16.</span> <span class="toc-text">4. 表格打包与序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E7%BB%93%E6%9E%9C%E7%94%9F%E6%88%90"><span class="toc-number">4.17.</span> <span class="toc-text">5. 结果生成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ColBERT"><span class="toc-number">5.</span> <span class="toc-text">ColBERT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">5.1.</span> <span class="toc-text">创新点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">5.2.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">5.3.</span> <span class="toc-text">解决的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ColBERT-%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B%EF%BC%9A%E8%AF%A6%E7%BB%86%E5%92%8C%E7%94%9F%E5%8A%A8%E7%9A%84%E8%AE%B2%E8%A7%A3"><span class="toc-number">5.4.</span> <span class="toc-text">ColBERT 使用流程示例：详细和生动的讲解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">5.5.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%A6%BB%E7%BA%BF%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%96%E7%A0%81%E6%96%87%E6%A1%A3"><span class="toc-number">5.6.</span> <span class="toc-text">1. 离线预处理和编码文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%9F%A5%E8%AF%A2%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%96%E7%A0%81"><span class="toc-number">5.7.</span> <span class="toc-text">2. 查询预处理和编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%BB%B6%E8%BF%9F%E4%BA%A4%E4%BA%92%E5%92%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">5.8.</span> <span class="toc-text">3. 延迟交互和相似度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%96%87%E6%A1%A3%E6%8E%92%E5%BA%8F%E5%92%8C%E6%A3%80%E7%B4%A2"><span class="toc-number">5.9.</span> <span class="toc-text">4. 文档排序和检索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E5%8A%A8%E5%BD%A2%E8%B1%A1%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.10.</span> <span class="toc-text">生动形象的示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ColBERT-v2"><span class="toc-number">6.</span> <span class="toc-text">ColBERT v2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction%E7%9A%84%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E5%92%8C%E4%BC%98%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction的主要改进和优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction-%E7%9A%84%E5%85%A8%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">6.2.</span> <span class="toc-text">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction 的全流程示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF-1"><span class="toc-number">6.3.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%A6%BB%E7%BA%BF%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%96%E7%A0%81%E6%96%87%E6%A1%A3-1"><span class="toc-number">6.4.</span> <span class="toc-text">1. 离线预处理和编码文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%9C%A8%E7%BA%BF%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E5%92%8C%E6%A3%80%E7%B4%A2"><span class="toc-number">6.5.</span> <span class="toc-text">2. 在线查询处理和检索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">6.6.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DPR"><span class="toc-number">7.</span> <span class="toc-text">DPR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E5%9C%BA%E6%99%AF"><span class="toc-number">7.1.</span> <span class="toc-text">示例场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-1-%E6%9F%A5%E8%AF%A2%E7%BC%96%E7%A0%81"><span class="toc-number">7.2.</span> <span class="toc-text">步骤 1: 查询编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-2-%E6%96%87%E6%A1%A3%E5%BA%93%E7%BC%96%E7%A0%81"><span class="toc-number">7.3.</span> <span class="toc-text">步骤 2: 文档库编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-3-%E5%90%91%E9%87%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">7.4.</span> <span class="toc-text">步骤 3: 向量相似度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-4-%E9%80%89%E6%8B%A9%E9%A1%B6%E9%83%A8%E6%96%87%E6%A1%A3"><span class="toc-number">7.5.</span> <span class="toc-text">步骤 4: 选择顶部文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-5-%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90"><span class="toc-number">7.6.</span> <span class="toc-text">步骤 5: 答案生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-6-%E8%BE%93%E5%87%BA%E6%9C%80%E7%BB%88%E7%AD%94%E6%A1%88"><span class="toc-number">7.7.</span> <span class="toc-text">步骤 6: 输出最终答案</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAFT"><span class="toc-number">8.</span> <span class="toc-text">RAFT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">9.</span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        LLM for Table data enhancement (CN)
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">YuhangWu</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-08-13T20:40:32.000Z" class="dt-published" itemprop="datePublished">2024-08-13</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/CN-Blog/" rel="tag">CN Blog</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from the author. For inquiries, please contact me at yuhang.wu-4 [at] postgrad.manchester.ac.uk.</p>
<p>随着大型语言模型（LLM）在自然语言处理（NLP）领域的广泛应用，它们在各种任务中展现了显著的能力，包括文本生成、问答系统和情感分析。然而，尽管LLM在处理非结构化数据方面表现优异，它们在处理结构化数据，特别是表格数据时，仍面临诸多挑战。表格数据的结构化特性和丰富的语义信息对LLM提出了更高的要求，传统的文本处理方法往往无法直接适用于此类数据。</p>
<p>本文旨在总结和讨论处理表格数据的关键技术和方法。我们深入分析了LLM在处理表格数据时重要的文献和方法，这些研究尝试解决LLM在处理表格数据时遇到的难题，包括表格数据的编码、查询和生成等方面。通过对ColBERT、ColBERTv2、DPR以及RAFT等技术的详细探讨，我们展示了当前LLM在表格数据处理领域的主要进展和创新。这些技术不仅提升了表格数据的理解和检索能力，也为未来的研究提供了重要的参考。</p>
<h2 id="LLM-for-Table"><a href="#LLM-for-Table" class="headerlink" title="LLM for Table"></a>LLM for Table</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.05121">Large Language Model for Table Processing</a></p>
<h3 id="1-LLMs表格处理方法分类"><a href="#1-LLMs表格处理方法分类" class="headerlink" title="1. LLMs表格处理方法分类"></a>1. LLMs表格处理方法分类</h3><p>在处理表格数据时，研究主要集中在训练和提示这两大方法上，具体包括：</p>
<ol>
<li><p><strong>基于训练的方法</strong>：</p>
<ul>
<li><strong>任务特定微调</strong>：例如TaPas和TaBERT，通过调整模型结构和训练目标来提升表格任务的性能。</li>
<li><strong>指令微调</strong>：如TableLlama和Table-GPT，通过在多个数据集上的微调，改进模型在未见任务上的表现。</li>
<li><strong>检索增强方法</strong>：如ITR和LI-RAGE，将大型表格分割成子表，并联合训练检索器和阅读器。</li>
</ul>
</li>
<li><p><strong>基于提示的方法</strong>：</p>
<ul>
<li><strong>表格序列化</strong>：将表格转换为线性文本格式，使LLMs更容易处理。</li>
<li><strong>少样本学习的示例选择</strong>：挑选与目标任务最相关的示例，以提高模型性能。</li>
</ul>
</li>
<li><p><strong>基于代理的方法</strong>：</p>
<ul>
<li><strong>复杂任务分解</strong>：例如DIN-SQL，通过将复杂任务分解为更小的子任务来提升准确性。</li>
<li><strong>动作定义</strong>：将软件工具的API抽象为动作，方便LLMs调用。</li>
<li><strong>反思和修正</strong>：通过生成多个推理路径并选择最一致的答案，或通过自我修正来提高模型的准确性。</li>
<li><strong>多任务框架</strong>：例如StructGPT，能够处理多种表格任务。</li>
</ul>
</li>
</ol>
<h3 id="具体方法归纳"><a href="#具体方法归纳" class="headerlink" title="具体方法归纳"></a>具体方法归纳</h3><ol>
<li><p><strong>任务特定微调</strong>：</p>
<ul>
<li><strong>TaPas</strong>：扩展BERT模型结构，进行表格预训练和微调。</li>
<li><strong>TaBERT</strong>：编码与输入语句相关的表格内容，并使用垂直注意力机制。</li>
<li><strong>TURL</strong>：将表格组件的信息编码为单独的输入嵌入，并将其融合在一起。</li>
</ul>
</li>
<li><p><strong>指令微调</strong>：</p>
<ul>
<li><strong>Table-GPT</strong>：使用合成-增强的方法构建指令微调数据集。</li>
<li><strong>TableLlama</strong>：利用现有数据集的真实数据进行指令微调。</li>
<li><strong>Magicoder</strong>：收集开源代码片段，生成编程问题和解决方案进行指令微调。</li>
</ul>
</li>
<li><p><strong>检索增强方法</strong>：</p>
<ul>
<li><strong>ITR</strong>：将大型表格分割成子表，并联合训练检索器和阅读器。</li>
<li><strong>DB-GPT</strong>：支持多种功能，如检索增强生成、微调和代理。</li>
</ul>
</li>
<li><p><strong>表格序列化</strong>：</p>
<ul>
<li>将表格内容线性化，插入列分隔符。</li>
<li>表格模式可以用普通文本或CREATE TABLE语句表示。</li>
</ul>
</li>
<li><p><strong>少样本学习的示例选择</strong>：</p>
<ul>
<li>选择与目标任务最相关的示例，平衡质量和数量。</li>
</ul>
</li>
<li><p><strong>复杂任务分解</strong>：</p>
<ul>
<li><strong>DIN-SQL</strong>：将text-to-SQL任务分解为子任务，生成中间子查询。</li>
</ul>
</li>
<li><p><strong>动作定义</strong>：</p>
<ul>
<li><strong>SheetCopilot</strong>：将现有电子表格软件API建模为原子动作，通过嵌入和聚类方法设计。</li>
<li><strong>ReAcTable</strong>：扩展ReAct框架，定义三种动作：生成SQL查询、生成Python代码和直接回答问题。</li>
</ul>
</li>
<li><p><strong>反思和修正</strong>：</p>
<ul>
<li>生成多个推理路径并选择最一致的答案。</li>
<li>采用提案和修正机制，反思并改进过去的动作。</li>
</ul>
</li>
<li><p><strong>多任务框架</strong>：</p>
<ul>
<li><strong>StructGPT</strong>：通过开发针对网页表格、数据库和知识图谱的三种动作，解决多种表格任务。</li>
</ul>
</li>
</ol>
<p>这篇综述系统地总结了LLMs在表格处理任务中的最新进展和具体方法，并为未来的研究和应用提供了参考。</p>
<h2 id="LLM-on-Tabular-Data-Retriever"><a href="#LLM-on-Tabular-Data-Retriever" class="headerlink" title="LLM on Tabular Data (Retriever)"></a>LLM on Tabular Data (Retriever)</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.17944">Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding</a></p>
<h3 id="5-2-大型语言模型在问答任务中的普遍能力"><a href="#5-2-大型语言模型在问答任务中的普遍能力" class="headerlink" title="5.2 大型语言模型在问答任务中的普遍能力"></a>5.2 大型语言模型在问答任务中的普遍能力</h3><p>表8列出了研究大型语言模型（LLMs）在问答和推理任务中的效果的论文及所探讨的模型。虽然GPT-3.5和GPT-4是最受欢迎的LLM，但这些模型并未特别优化用于表格任务。然而，结合一些提示工程（prompt engineering）技巧（如Chain of Thought，CoT），它们在执行复杂的表格推理任务时表现出色。</p>
<h3 id="数值问答（Numerical-QA）"><a href="#数值问答（Numerical-QA）" class="headerlink" title="数值问答（Numerical QA）"></a>数值问答（Numerical QA）</h3><p>数值问答任务涉及数学推理，例如，“美国运通的每笔交易的平均支付量是多少？”在许多实际应用中，如处理财务文件和年报，这种数学推理任务非常常见。Akhtar等（2023）发现，FlanT5和GPT-3.5在各种数值推理任务中表现优于其他模型。在DOCMATH-EVAL（Zhao等，2023d）数据集上，使用CoT的GPT-4显著优于其他LLM，而开源LLM（如LLaMa-2、Vicuna、Mistral、Starcoder、MPT、Qwen、AquilaChat2等）则表现较差。</p>
<h3 id="Text2SQL"><a href="#Text2SQL" class="headerlink" title="Text2SQL"></a>Text2SQL</h3><p>Liu等（2023c）设计了一个问题匹配器，识别三种关键词类型：1）列名相关术语，2）限制相关短语（如“前十名”），3）算法或模块关键词。一旦识别了这些关键词，该模块将与每列相关的具体限制合并成统一的组合，然后将其与第三种关键词所指示的SQL算法或模块匹配。Zhang等（2023d）选择了一种更直接的方法，即让LLaMa-2基于问题和表模式生成SQL语句。Sun等（2023b）在Text2SQL任务上对PaLM-2进行了微调，在Spider数据集上取得了显著成绩。OpenTab（Kong等，2024）开发了一个基于LLM的开放域表格问答框架，结合了一个SQL生成模块。今天在Spider上的顶级模型包括Dong等（2023）、Gao等（2024）和Pourreza &amp; Rafiei（2023），他们都建立在OpenAI的GPT模型之上。SQL生成在业界很受欢迎，许多开源微调模型也可用。</p>
<h3 id="模型大小对性能的影响"><a href="#模型大小对性能的影响" class="headerlink" title="模型大小对性能的影响"></a>模型大小对性能的影响</h3><p>Chen（2023）发现模型大小确实重要：在WebTableQuestions上，比较6.7B和175B的GPT-3模型时，较小的模型仅达到了较大模型得分的一半。在TabFact上，他们发现较小的模型（&lt;&#x3D;6.7B）的准确性几乎是随机的。</p>
<h3 id="微调还是不微调？"><a href="#微调还是不微调？" class="headerlink" title="微调还是不微调？"></a>微调还是不微调？</h3><p>一些较大的模型在各种表格任务（包括问答和事实验证任务）上进行了微调。Li等（2023d）发现，微调总是有助于提高各种表格任务的性能，尤其是在零样本设置中改进最为显著。Ye等（2023b）使用PASTA（Gu等，2022）模型在TabFact上得分更高（93.00%），相比之下，GPT-3 Codex（code-davinci-002）得分为85.60%。PASTA在一个由维基百科表格组成的120万条目合成语料库上进行了预训练，用于六种类型的句子-表格填空任务。这表明在使用微调的LLM在表格任务上仍有一些好处。</p>
<p>然而，与在预测和生成任务上工作的其他方法相比，微调并不常见。这可能是因为LLMs（如GPT-3.5，GPT-4）在开箱即用的问答任务中表现良好。在Spider上的SQL生成中，DIN-SQL（Pourreza &amp; Rafiei，2023）和DAIL-SQL（Gao等，2024）是使用GPT-4的推理技术，超越了以前微调的较小模型。有趣的是，在Gao等（2024）的论文中，作者在Text2SQL任务上微调了一个Llama 2 13B模型，但该模型并没有超过未微调的GPT-4模型。相反，许多使用LLM进行表格理解任务的论文专注于调整序列化、提示工程、搜索和检索以及端到端管道（用户界面）等方面的内容，我们将在下一部分进一步描述。</p>
<h3 id="5-3-表格问答的关键组成部分"><a href="#5-3-表格问答的关键组成部分" class="headerlink" title="5.3 表格问答的关键组成部分"></a>5.3 表格问答的关键组成部分</h3><p>在最简单的问答架构中，一个LLM接收一个输入提示（查询和序列化表格），并返回一个答案。在更复杂的架构中，系统可能连接到外部数据库或程序。大多数情况下，知识库可能无法适应LLM的上下文长度或内存。因此，LLM在表格问答</p>
<p>中的独特挑战包括：查询意图的消歧、搜索和检索、输出类型和格式，以及需要程序之间迭代调用的多轮设置。在本节中，我们将进一步描述这些组件。</p>
<h3 id="5-3-1-查询意图消歧"><a href="#5-3-1-查询意图消歧" class="headerlink" title="5.3.1 查询意图消歧"></a>5.3.1 查询意图消歧</h3><p>Zha等（2023）引入了命令链（Chain-of-command，CoC）的概念，将用户输入转换为一系列中间命令操作。例如，一个LLM需要首先检查任务是否需要检索、数学推理、表格操作，以及如果指令太模糊无法回答问题。他们构建了一个命令链指令数据集，以微调LLMs生成这些命令。Deng等（2022b）建议将问答任务分为三个子任务：澄清需求预测（CNP）以确定是否需要提出澄清不确定性的问；澄清问题生成（CQG），如果CNP检测到需要澄清，则生成一个澄清问题作为响应；以及对话问答（CQA），如果不需要澄清，则直接生成答案作为响应。他们训练了一个UniPCQA模型，通过多任务学习统一了问答中的所有子任务。</p>
<hr>
<h3 id="搜索和检索方法归纳总结"><a href="#搜索和检索方法归纳总结" class="headerlink" title="搜索和检索方法归纳总结"></a>搜索和检索方法归纳总结</h3><h3 id="1-检索模块性能提升"><a href="#1-检索模块性能提升" class="headerlink" title="1. 检索模块性能提升"></a>1. 检索模块性能提升</h3><ul>
<li><strong>Retriever Module</strong>: 返回最相关的前n个文档，性能越好，最终准确性越高（Zhao et al., 2023d）。<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2311.09805">论文链接</a></li>
</ul>
<h3 id="2-表格采样方法"><a href="#2-表格采样方法" class="headerlink" title="2. 表格采样方法"></a>2. 表格采样方法</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.09039">TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning</a></p>
<ul>
<li><strong>多种采样方法</strong>: 包括行和列的采样以及表格打包方法（Sui et al., 2023c）。<ul>
<li><strong>基于查询的采样</strong>: 检索与问题语义相似度最高的行，效果最好。</li>
<li><strong>无采样、聚类、随机、均匀采样、内容快照方法</strong>: 相对较弱。</li>
</ul>
</li>
</ul>
<h3 id="3-表格排名方法"><a href="#3-表格排名方法" class="headerlink" title="3. 表格排名方法"></a>3. 表格排名方法</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.07306">C3: Zero-shot Text-to-SQL with ChatGPT</a></p>
<ul>
<li><strong>ChatGPT 排名表格</strong>: 生成十组检索结果，每组包含前四个表格，然后选择出现最频繁的一组（Dong et al., 2023）。</li>
<li><strong>列排名</strong>: 根据与问题的相关性对所有列进行排名，匹配列名与问题词，或优先处理外键以帮助更准确的召回（Dong et al., 2023）。</li>
</ul>
<h3 id="4-三步检索架构"><a href="#4-三步检索架构" class="headerlink" title="4. 三步检索架构"></a>4. 三步检索架构</h3><ul>
<li><strong>Dense Table Retrieval (DTR)</strong>: 基于RoBERTa的双编码器模型，识别与查询最相关的表格（Sundar &amp; Heck, 2023）。<br><a target="_blank" rel="noopener" href="https://aclanthology.org/2023.nlp4convai-1.6.pdf">cTBLS: Augmenting Large Language Models with Conversational Tables</a></li>
<li><strong>Coarse System State Tracking</strong>: 使用三元组损失训练，对单元格进行排名（Sundar &amp; Heck, 2023）。</li>
<li><strong>自然语言响应生成</strong>: 基于GPT-3.5，结合对话历史、排名的知识来源和查询生成响应（Sundar &amp; Heck, 2023）。</li>
</ul>
<h3 id="5-稠密和稀疏检索器结合"><a href="#5-稠密和稀疏检索器结合" class="headerlink" title="5. 稠密和稀疏检索器结合"></a>5. 稠密和稀疏检索器结合</h3><ul>
<li><strong>Ada Embedding4 和 Contriever</strong>: 作为稠密检索器（Zhao et al., 2023d）。</li>
<li><strong>BM25</strong>: 作为稀疏检索器（Zhao et al., 2023d）。</li>
<li><strong>综合使用</strong>: 提取最相关的文本和表格证据。</li>
</ul>
<h3 id="6-获取额外信息的方法"><a href="#6-获取额外信息的方法" class="headerlink" title="6. 获取额外信息的方法"></a>6. 获取额外信息的方法</h3><p><a target="_blank" rel="noopener" href="https://www.vldb.org/pvldb/vol17/p1132-gao.pdf">Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation</a></p>
<p>Code: <a target="_blank" rel="noopener" href="https://github.com/BeachWang/DAIL-SQL">Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation</a></p>
<ul>
<li><strong>策划样本用于上下文学习</strong>（Gao et al., 2024）<ul>
<li><strong>随机选择</strong>: 随机选择k个示例。</li>
<li><strong>问题相似度选择</strong>: 基于与问题Q的语义相似度选择k个示例。</li>
<li><strong>掩蔽问题相似度选择</strong>: 掩蔽问题中的领域特定信息后进行相似度选择。</li>
<li><strong>查询相似度选择</strong>: 选择与目标SQL查询s*相似的k个示例，使用另一个模型生成近似SQL查询s’。</li>
</ul>
</li>
</ul>
<h3 id="7-手动和随机示例选择"><a href="#7-手动和随机示例选择" class="headerlink" title="7. 手动和随机示例选择"></a>7. 手动和随机示例选择</h3><ul>
<li><strong>手动策划和随机选择</strong>（Narayan et al., 2022）</li>
</ul>
<h2 id="LI-RAGE"><a href="#LI-RAGE" class="headerlink" title="LI-RAGE"></a>LI-RAGE</h2><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2023.acl-short.133.pdf">LI-RAGE: L ate I nteraction R etrieval A ugmented Generation with Explicit Signals for Open-Domain Table Question Answering</a></p>
<h3 id="文章内容简短总结"><a href="#文章内容简短总结" class="headerlink" title="文章内容简短总结"></a>文章内容简短总结</h3><p>LI-RAGE框架是一种针对开放领域表格问答（TableQA）的新方法。该方法通过结合后期交互检索（Late Interaction，LI）模型和显式信号的检索增强生成（RAGE）损失，显著提升了表格问答的性能。与传统的检索-阅读器（Retriever-Reader）管道相比，LI-RAGE通过以下改进提供了更高的准确性和可靠性：</p>
<ol>
<li><strong>后期交互模型（LI）</strong>：使用ColBERT模型对问题和表格进行逐词编码，捕捉更细粒度的交互信息，从而提升表格检索效果。</li>
<li><strong>联合训练（RAGE损失）</strong>：将检索器和阅读器的信号结合进行联合训练，优化表格检索与答案生成的效果。</li>
<li><strong>二进制相关性标记</strong>：在生成答案前添加二进制相关性标记（yes&#x2F;no），以指示表格是否与问题相关，从而提高答案的可靠性。</li>
</ol>
<h3 id="示例问题"><a href="#示例问题" class="headerlink" title="示例问题"></a>示例问题</h3><p>考虑到问题“哪个国家的人口最多？”</p>
<h3 id="表格数据集"><a href="#表格数据集" class="headerlink" title="表格数据集"></a>表格数据集</h3><p>假设有如下表格数据：</p>
<p>表格1：</p>
<table>
<thead>
<tr>
<th>国家</th>
<th>人口</th>
</tr>
</thead>
<tbody><tr>
<td>中国</td>
<td>1,411百万</td>
</tr>
<tr>
<td>印度</td>
<td>1,366百万</td>
</tr>
<tr>
<td>美国</td>
<td>331百万</td>
</tr>
</tbody></table>
<p>表格2：</p>
<table>
<thead>
<tr>
<th>国家</th>
<th>面积</th>
</tr>
</thead>
<tbody><tr>
<td>俄罗斯</td>
<td>17百万 km²</td>
</tr>
<tr>
<td>加拿大</td>
<td>9.98百万 km²</td>
</tr>
<tr>
<td>中国</td>
<td>9.6百万 km²</td>
</tr>
</tbody></table>
<p>表格3：</p>
<table>
<thead>
<tr>
<th>城市</th>
<th>人口</th>
</tr>
</thead>
<tbody><tr>
<td>纽约</td>
<td>8百万</td>
</tr>
<tr>
<td>东京</td>
<td>14百万</td>
</tr>
<tr>
<td>上海</td>
<td>24百万</td>
</tr>
</tbody></table>
<h3 id="全流程示例"><a href="#全流程示例" class="headerlink" title="全流程示例"></a>全流程示例</h3><h4 id="1-表格检索（Retriever）"><a href="#1-表格检索（Retriever）" class="headerlink" title="1. 表格检索（Retriever）"></a>1. 表格检索（Retriever）</h4><p>检索器从表格语料库中选择与问题最相关的表格。在此示例中，检索器可能会选择表格1，因为它包含了国家与人口的相关信息。</p>
<p><strong>检索结果：</strong><br>选择表格1：</p>
<table>
<thead>
<tr>
<th>国家</th>
<th>人口</th>
</tr>
</thead>
<tbody><tr>
<td>中国</td>
<td>1,411百万</td>
</tr>
<tr>
<td>印度</td>
<td>1,366百万</td>
</tr>
<tr>
<td>美国</td>
<td>331百万</td>
</tr>
</tbody></table>
<h4 id="2-答案生成（Reader）"><a href="#2-答案生成（Reader）" class="headerlink" title="2. 答案生成（Reader）"></a>2. 答案生成（Reader）</h4><p>答案生成器模型将问题和检索到的表格作为输入，并生成答案。在此例中，答案生成器将问题“哪个国家的人口最多？”与表格1结合，通过识别关键词“国家”和“人口最多”，在表格中查找最大值对应的国家，生成答案：“中国”。</p>
<h4 id="3-二进制相关性标记（Binary-Relevance-Token）"><a href="#3-二进制相关性标记（Binary-Relevance-Token）" class="headerlink" title="3. 二进制相关性标记（Binary Relevance Token）"></a>3. 二进制相关性标记（Binary Relevance Token）</h4><p>为确保答案生成器选择的表格是可靠的，在生成答案前添加二进制相关性标记。在训练过程中，系统学习到从黄金表格生成的答案前有“yes”，而从非黄金表格生成的答案前有“no”。此例中，生成的答案是从黄金表格（表格1）中得出的，因此答案前有“yes”。</p>
<p><strong>最终输出：</strong><br>答案生成器输出：“yes 中国”。</p>
<h4 id="4-过滤与确定最终答案"><a href="#4-过滤与确定最终答案" class="headerlink" title="4. 过滤与确定最终答案"></a>4. 过滤与确定最终答案</h4><p>在推理阶段，如果答案生成器输出的前缀为“yes”，则表明该答案可靠。系统将优先选择标记为“yes”的答案；如果所有候选答案的前缀均为“no”，系统将依据答案生成器的置信度得分来选择最终答案。在此例中，系统识别到“yes”标记，确认该答案可靠，最终输出答案：“中国”。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>上述流程展示了开放领域表格问答系统从输入问题到生成最终答案的完整过程。通过LI-RAGE框架，系统不仅能够从大量表格数据中有效检索相关信息，还能通过二进制相关性标记确保答案的可靠性。</p>
<h2 id="TAP4LLM"><a href="#TAP4LLM" class="headerlink" title="TAP4LLM"></a>TAP4LLM</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.09039">TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning</a></p>
<h3 id="TAP4LLM方法详细总结"><a href="#TAP4LLM方法详细总结" class="headerlink" title="TAP4LLM方法详细总结"></a>TAP4LLM方法详细总结</h3><h3 id="大型语言模型在表格数据中的应用"><a href="#大型语言模型在表格数据中的应用" class="headerlink" title="大型语言模型在表格数据中的应用"></a>大型语言模型在表格数据中的应用</h3><p>随着自然语言处理领域大型语言模型（LLMs）的进展，研究者开始探索将这些模型应用于其他模态，如视觉和语音。然而，直接在表格领域使用传统LLMs面临两个主要挑战：</p>
<ol>
<li><strong>全局表理解</strong>：LLMs如GPT存在令牌长度限制，难以读取并理解整个大表格，从而限制了其对表格全局信息的理解。</li>
<li><strong>对表格领域的泛化</strong>：这些模型的训练过程主要针对自然语言，因此在处理表格数据时泛化能力较弱。</li>
</ol>
<p>尽管一些研究已经尝试将自然语言处理与表格数据分析相结合，LLMs在表格问答中的表现仍然受限。</p>
<h3 id="表格增强技术"><a href="#表格增强技术" class="headerlink" title="表格增强技术"></a>表格增强技术</h3><p>表格增强技术旨在提高机器学习模型的泛化性能和鲁棒性。为了提升LLMs在表格领域的表现，研究者进行了多种增强方法的探索，包括结构化知识、常识知识和分析性知识的结合。研究表明，利用领域特定的元数据或知识图能够显著提高LLMs对表格数据的理解能力。例如:</p>
<ul>
<li><strong>Jena et al. (2022)</strong> 提出了半自动地转换现有表格数据，创建多样化的自然语言推理实例，以提高零样本性能。</li>
<li><strong>He et al. (2023)</strong> 提出了一个多任务元数据模型，利用字段分布和知识图信息，准确推断表格的分析元数据，并展示了其在智能数据分析产品中的应用。</li>
</ul>
<h3 id="TAP4LLM的核心组件"><a href="#TAP4LLM的核心组件" class="headerlink" title="TAP4LLM的核心组件"></a>TAP4LLM的核心组件</h3><p>TAP4LLM通过以下三个核心组件解决了综合表格理解中的主要挑战：</p>
<ol>
<li><strong>表格采样</strong>：从表格中选择和提取与查询最相关的行和列。<ul>
<li><strong>基于规则的采样</strong>：通过预定义标准或规则，如随机采样、均匀采样和内容快照采样，进行表格采样。</li>
<li><strong>基于嵌入的采样</strong>：通过语义和上下文表示选择行和列，采用基于语义的采样、基于中心点的采样等方法。</li>
<li><strong>基于LLM的采样</strong>：利用强大的LLM来预测表格行和列的索引，但这种方法计算成本较高。</li>
</ul>
</li>
<li><strong>表格增强</strong>：通过添加外部知识和元数据丰富表格信息。<ul>
<li><strong>基于元数据的增强</strong>：包括维度&#x2F;度量、语义字段类型、表格大小、统计特征和标题层次结构等信息的添加。</li>
<li><strong>基于检索的增强</strong>：通过文档检索系统，从外部知识库中获取相关内容以减少幻觉或事实性错误。</li>
<li><strong>基于自一致性的增强</strong>：通过迭代生成和改进查询与响应，提高模型的推理能力。</li>
</ul>
</li>
<li><strong>表格打包与序列化</strong>：控制令牌分配，将表格和增强信息打包成适用于LLMs的序列。<ul>
<li>经验研究显示，子表长度与增强信息长度的比例在5:5和4:6之间时，通常带来最佳性能。</li>
<li>支持多种序列化格式，如HTML、XML、JSON、CSV、NL+Sep和Markdown等。</li>
</ul>
</li>
</ol>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>TAP4LLM通过表格采样、表格增强以及表格打包与序列化，解决了综合表格理解中的主要挑战，提升了LLMs在表格推理任务中的有效性。该方法不仅适用于表格建模，还能在金融、交通等领域中发挥重要作用，推动基于表格数据的研究。</p>
<h3 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h3><p>代码生成方法已被提出用于将自然语言查询转换为可执行代码或结构化表示（Cheng et al., 2023; Gemmell and Dalton, 2023）。这一研究方向具有重要性，但由于篇幅限制，本研究未深入探讨此主题。当前的实证研究主要集中在英语场景，对多语言能力的讨论也将留待未来研究中进行。</p>
<h3 id="示例：使用TAP4LLM进行表格数据分析"><a href="#示例：使用TAP4LLM进行表格数据分析" class="headerlink" title="示例：使用TAP4LLM进行表格数据分析"></a>示例：使用TAP4LLM进行表格数据分析</h3><p>假设存在一个包含某公司过去几年季度财务报告的金融数据表格。表格列包括年份、季度、收入、支出、净利润和资产负债率。目标是通过自然语言查询“该公司在过去五年中的季度净利润趋势如何？”来生成准确的分析。</p>
<h3 id="1-表格采样"><a href="#1-表格采样" class="headerlink" title="1. 表格采样"></a>1. 表格采样</h3><p><strong>初始表格（T）</strong>：</p>
<table>
<thead>
<tr>
<th>年份</th>
<th>季度</th>
<th>收入</th>
<th>支出</th>
<th>净利润</th>
<th>资产负债率</th>
</tr>
</thead>
<tbody><tr>
<td>2019</td>
<td>Q1</td>
<td>1000</td>
<td>800</td>
<td>200</td>
<td>50%</td>
</tr>
<tr>
<td>2019</td>
<td>Q2</td>
<td>1100</td>
<td>850</td>
<td>250</td>
<td>48%</td>
</tr>
<tr>
<td>2019</td>
<td>Q3</td>
<td>1050</td>
<td>820</td>
<td>230</td>
<td>49%</td>
</tr>
<tr>
<td>2019</td>
<td>Q4</td>
<td>1200</td>
<td>900</td>
<td>300</td>
<td>47%</td>
</tr>
<tr>
<td>2020</td>
<td>Q1</td>
<td>1300</td>
<td>950</td>
<td>350</td>
<td>46%</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>2023</td>
<td>Q4</td>
<td>1600</td>
<td>1200</td>
<td>400</td>
<td>45%</td>
</tr>
</tbody></table>
<p>为回答“该公司在过去五年中的季度净利润趋势如何？”这一查询，需要对表格进行采样。</p>
<p><strong>表格采样步骤</strong>：</p>
<ul>
<li><strong>基于规则的采样</strong>：选择最近五年的数据（2019-2023）。</li>
<li><strong>基于嵌入的采样</strong>：使用语义采样，选择与“净利润”相关的行和列。</li>
</ul>
<p><strong>采样后的子表（T’）</strong>：</p>
<table>
<thead>
<tr>
<th>年份</th>
<th>季度</th>
<th>净利润</th>
</tr>
</thead>
<tbody><tr>
<td>2019</td>
<td>Q1</td>
<td>200</td>
</tr>
<tr>
<td>2019</td>
<td>Q2</td>
<td>250</td>
</tr>
<tr>
<td>2019</td>
<td>Q3</td>
<td>230</td>
</tr>
<tr>
<td>2019</td>
<td>Q4</td>
<td>300</td>
</tr>
<tr>
<td>2020</td>
<td>Q1</td>
<td>350</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>2023</td>
<td>Q4</td>
<td>400</td>
</tr>
</tbody></table>
<h3 id="2-表格增强"><a href="#2-表格增强" class="headerlink" title="2. 表格增强"></a>2. 表格增强</h3><p><strong>元数据增强</strong>：</p>
<ul>
<li><strong>维度&#x2F;度量</strong>：添加“净利润”作为度量值。</li>
<li><strong>统计特征</strong>：计算各季度的平均净利润、最大值和最小值。</li>
</ul>
<p><strong>检索增强</strong>：</p>
<ul>
<li><strong>文档参考</strong>：从外部数据库检索相关的行业分析报告，解释财务术语如“净利润”的定义及其影响因素。</li>
<li><strong>术语解释</strong>：提供“资产负债率”的解释，并说明其与净利润的关系。</li>
</ul>
<p><strong>自一致性增强</strong>：</p>
<ul>
<li>使用自提示方法生成初步分析结果，并将这些结果重新输入模型以进一步提高准确性。</li>
</ul>
<h3 id="3-表格打包与序列化"><a href="#3-表格打包与序列化" class="headerlink" title="3. 表格打包与序列化"></a>3. 表格打包与序列化</h3><p><strong>序列化格式</strong>：选择JSON格式进行打包。</p>
<p><strong>打包后的序列</strong>：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;该公司在过去五年中的季度净利润趋势如何？&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;年份&quot;</span><span class="punctuation">:</span> <span class="number">2019</span><span class="punctuation">,</span> <span class="attr">&quot;季度&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Q1&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;净利润&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;年份&quot;</span><span class="punctuation">:</span> <span class="number">2019</span><span class="punctuation">,</span> <span class="attr">&quot;季度&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Q2&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;净利润&quot;</span><span class="punctuation">:</span> <span class="number">250</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;年份&quot;</span><span class="punctuation">:</span> <span class="number">2019</span><span class="punctuation">,</span> <span class="attr">&quot;季度&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Q3&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;净利润&quot;</span><span class="punctuation">:</span> <span class="number">230</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;年份&quot;</span><span class="punctuation">:</span> <span class="number">2019</span><span class="punctuation">,</span> <span class="attr">&quot;季度&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Q4&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;净利润&quot;</span><span class="punctuation">:</span> <span class="number">300</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;年份&quot;</span><span class="punctuation">:</span> <span class="number">2020</span><span class="punctuation">,</span> <span class="attr">&quot;季度&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Q1&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;净利润&quot;</span><span class="punctuation">:</span> <span class="number">350</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;年份&quot;</span><span class="punctuation">:</span> <span class="number">2023</span><span class="punctuation">,</span> <span class="attr">&quot;季度&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Q4&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;净利润&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;度量值&quot;</span><span class="punctuation">:</span> <span class="string">&quot;净利润&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;统计特征&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;平均净利润&quot;</span><span class="punctuation">:</span> <span class="number">275</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;最大净利润&quot;</span><span class="punctuation">:</span> <span class="number">400</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;最小净利润&quot;</span><span class="punctuation">:</span> <span class="number">200</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;augmentation&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;文档参考&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;相关行业分析报告链接&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;术语解释&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;资产负债率&quot;</span><span class="punctuation">:</span> <span class="string">&quot;资产负债率是衡量公司负债与资产比率的指标，反映公司财务健康状况。&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="4-最终分析结果"><a href="#4-最终分析结果" class="headerlink" title="4. 最终分析结果"></a>4. 最终分析结果</h3><p>使用TAP4LLM预处理后的数据和增强信息，生成最终的分析报告：</p>
<p><strong>生成的分析报告</strong>：</p>
<p>“根据该公司的财务数据，过去五年的季度净利润呈现逐步增长的趋势，从2019年Q1的200万增长到2023年Q4的400万。平均净利润为275万，最大值为400万，最小值为200万。此增长趋势反映了公司在财务管理和市场策略上的成功。此外，资产负债率从2019年的50%降低到2023年的45%，进一步显示了公司的财务健康状况。”</p>
<p>通过以上步骤，展示了如何利用TAP4LLM方法处理和分析表格数据，从而生成高质量的自然语言报告。</p>
<h3 id="例子：使用TAP4LLM回答金融数据问题"><a href="#例子：使用TAP4LLM回答金融数据问题" class="headerlink" title="例子：使用TAP4LLM回答金融数据问题"></a>例子：使用TAP4LLM回答金融数据问题</h3><p>假设有一个包含股票价格每日记录的金融数据表格，问题是关于特定时间段内某股票的平均价格。</p>
<h3 id="1-输入表格和问题"><a href="#1-输入表格和问题" class="headerlink" title="1. 输入表格和问题"></a>1. 输入表格和问题</h3><p><strong>表格T</strong>:</p>
<table>
<thead>
<tr>
<th>日期</th>
<th>股票代码</th>
<th>开盘价</th>
<th>收盘价</th>
<th>最高价</th>
<th>最低价</th>
<th>成交量</th>
</tr>
</thead>
<tbody><tr>
<td>2024-01-01</td>
<td>AAPL</td>
<td>150</td>
<td>155</td>
<td>157</td>
<td>149</td>
<td>1000000</td>
</tr>
<tr>
<td>2024-01-02</td>
<td>AAPL</td>
<td>156</td>
<td>158</td>
<td>159</td>
<td>155</td>
<td>1200000</td>
</tr>
<tr>
<td>2024-01-03</td>
<td>AAPL</td>
<td>157</td>
<td>159</td>
<td>160</td>
<td>156</td>
<td>1100000</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>2024-01-01</td>
<td>MSFT</td>
<td>210</td>
<td>215</td>
<td>217</td>
<td>209</td>
<td>900000</td>
</tr>
<tr>
<td>2024-01-02</td>
<td>MSFT</td>
<td>216</td>
<td>218</td>
<td>219</td>
<td>215</td>
<td>950000</td>
</tr>
<tr>
<td>2024-01-03</td>
<td>MSFT</td>
<td>217</td>
<td>220</td>
<td>221</td>
<td>216</td>
<td>920000</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody></table>
<p><strong>问题Q</strong>:<br>“AAPL股票在2024年1月1日至2024年1月3日的平均收盘价是多少？”</p>
<h3 id="2-表格采样"><a href="#2-表格采样" class="headerlink" title="2. 表格采样"></a>2. 表格采样</h3><p>根据问题Q，只需提取AAPL股票在指定日期范围内的相关数据。可以使用基于语义的采样方法，通过计算查询Q与表格每行的相似度，选出相关行。</p>
<p><strong>采样后的子表T’</strong>:</p>
<table>
<thead>
<tr>
<th>日期</th>
<th>股票代码</th>
<th>开盘价</th>
<th>收盘价</th>
<th>最高价</th>
<th>最低价</th>
<th>成交量</th>
</tr>
</thead>
<tbody><tr>
<td>2024-01-01</td>
<td>AAPL</td>
<td>150</td>
<td>155</td>
<td>157</td>
<td>149</td>
<td>1000000</td>
</tr>
<tr>
<td>2024-01-02</td>
<td>AAPL</td>
<td>156</td>
<td>158</td>
<td>159</td>
<td>155</td>
<td>1200000</td>
</tr>
<tr>
<td>2024-01-03</td>
<td>AAPL</td>
<td>157</td>
<td>159</td>
<td>160</td>
<td>156</td>
<td>1100000</td>
</tr>
</tbody></table>
<h3 id="3-表格增强"><a href="#3-表格增强" class="headerlink" title="3. 表格增强"></a>3. 表格增强</h3><p>为帮助LLM更好地理解表格数据，可以引入一些增强信息，如股票的历史数据统计信息或相关术语解释。</p>
<p><strong>增强信息</strong>:</p>
<ul>
<li>维度&#x2F;度量：股票代码为维度，收盘价为度量。</li>
<li>术语解释：解释“收盘价”是指股票在交易日结束时的价格。</li>
<li>历史统计信息：过去一年的平均收盘价、最高价、最低价等。</li>
</ul>
<h3 id="4-表格打包与序列化"><a href="#4-表格打包与序列化" class="headerlink" title="4. 表格打包与序列化"></a>4. 表格打包与序列化</h3><p>将采样后的表格和增强信息打包为一个序列，适用于LLM的输入。</p>
<p><strong>打包后的输入</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">问题: 请问AAPL股票在2024年1月1日至2024年1月3日的平均收盘价是多少？</span><br><span class="line">表格:</span><br><span class="line">| 日期       | 股票代码 | 收盘价 |</span><br><span class="line">|------------|----------|--------|</span><br><span class="line">| 2024-01-01 | AAPL     | 155    |</span><br><span class="line">| 2024-01-02 | AAPL     | 158    |</span><br><span class="line">| 2024-01-03 | AAPL     | 159    |</span><br><span class="line"></span><br><span class="line">增强信息:</span><br><span class="line">- 股票代码为维度，收盘价为度量。</span><br><span class="line">- 收盘价是指股票在交易日结束时的价格。</span><br><span class="line">- 过去</span><br><span class="line"></span><br><span class="line">一年AAPL的平均收盘价为160，最高价为165，最低价为150。</span><br></pre></td></tr></table></figure>

<h3 id="5-结果生成"><a href="#5-结果生成" class="headerlink" title="5. 结果生成"></a>5. 结果生成</h3><p>使用LLM来计算平均值并生成答案。</p>
<p><strong>答案</strong>:</p>
<p>“根据提供的数据，AAPL股票在2024年1月1日至2024年1月3日的平均收盘价为157.33。”</p>
<h2 id="ColBERT"><a href="#ColBERT" class="headerlink" title="ColBERT"></a><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a></h2><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><ol>
<li><strong>延迟交互框架</strong>：<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>引入了一种延迟交互架构，通过将查询和文档的编码过程分离，并在查询和文档分别编码后再进行相似度计算。这使得可以预先计算文档的表示，减少了在线查询时的计算量。</li>
<li><strong>最大相似度操作（MaxSim）</strong>：在评估查询和文档之间的相关性时，<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>采用了最大相似度操作（MaxSim），即每个查询嵌入与文档嵌入之间的最大余弦相似度或L2距离，并将这些最大相似度值求和。这种机制既简单又高效。</li>
<li><strong>BERT编码器共享</strong>：<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>在查询和文档编码器之间共享一个BERT模型，但通过在查询和文档前分别加上特殊标记（[Q]和[D]）来区分输入。这种方法既节省了计算资源，又保持了模型的上下文理解能力。</li>
<li><strong>查询和文档的分段和过滤</strong>：文档编码器在处理文档时，会过滤掉标点符号的嵌入，以减少计算量和存储空间。</li>
<li><strong>基于向量相似性的检索</strong>：利用现有的向量相似性搜索库（如faiss），<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>可以通过高效的剪枝操作实现从大型文档集合中的端到端检索。</li>
</ol>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li><strong>计算效率高</strong>：通过预计算文档表示和使用延迟交互机制，ColBERT在处理查询时大大减少了计算量，相对于其他基于BERT的模型，其速度提高了两个数量级。</li>
<li><strong>高效的空间利用</strong>：通过对文档嵌入进行归一化和降维处理，ColBERT显著减少了存储空间需求，使其能够在实际应用中更加可行。</li>
<li><strong>强大的扩展性</strong>：ColBERT的架构设计使其能够在不牺牲精度的情况下处理大规模文档集合，特别是在使用向量相似性搜索进行剪枝时，其检索效率大大提高。</li>
<li><strong>端到端检索能力</strong>：ColBERT不仅可以用于重新排序预检索的文档集，还可以直接从大型文档集合中进行端到端检索，提高了整体检索系统的召回率和精度。</li>
</ol>
<h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><ol>
<li><strong>计算成本高</strong>：传统基于BERT的排序模型在查询-文档对的计算上非常耗时，ColBERT通过引入延迟交互和预计算机制，大大降低了在线计算成本。</li>
<li><strong>响应时间长</strong>：高计算成本导致的长查询响应时间对用户体验有负面影响。ColBERT通过更高效的计算和检索机制，显著减少了查询延迟。</li>
<li><strong>存储空间大</strong>：深度语言模型通常需要大量存储空间来保存文档表示，ColBERT通过归一化和降维处理减少了存储需求。</li>
<li><strong>检索精度与效率的权衡</strong>：现有方法在提高检索效率时往往牺牲精度，ColBERT通过高效的延迟交互和向量相似性搜索，在不降低精度的情况下提高了检索效率。</li>
</ol>
<h3 id="ColBERT-使用流程示例：详细和生动的讲解"><a href="#ColBERT-使用流程示例：详细和生动的讲解" class="headerlink" title="ColBERT 使用流程示例：详细和生动的讲解"></a>ColBERT 使用流程示例：详细和生动的讲解</h3><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>假设你正在使用一个学术论文数据库，里面有数百万篇学术论文。你正在研究“机器学习的好处”，想找到最相关的论文。这就是ColBERT可以帮助你的地方。</p>
<h3 id="1-离线预处理和编码文档"><a href="#1-离线预处理和编码文档" class="headerlink" title="1. 离线预处理和编码文档"></a>1. 离线预处理和编码文档</h3><p>在任何查询发生之前，我们先对数据库中的每一篇论文进行预处理和编码。这是一个离线过程，类似于图书馆对所有书籍进行分类和编号。</p>
<ol>
<li><strong>分割文档</strong>：将每篇论文分解成单词，比如“机器学习是一种数据分析的方法，可以自动构建分析模型”会被分解成“机器”、“学习”、“是”、“一种”、“数据”、“分析”……。</li>
<li><strong>添加标记</strong>：在每篇论文的开头加上特殊标记，比如“[D]”，以表明这是一个文档。</li>
<li><strong>BERT编码</strong>：使用BERT模型对每个单词进行编码，将它们转化为具有上下文意义的向量表示。这就像是为每个单词生成一个独特的数字签名。</li>
<li><strong>过滤无关信息</strong>：去除标点符号等无关信息，保持重要的单词。</li>
<li><strong>归一化和降维</strong>：对这些向量进行归一化处理，使它们的表示更加紧凑和高效，类似于将大文件压缩成小文件，便于存储和处理。</li>
<li><strong>存储嵌入</strong>：将处理后的向量存储到数据库中，以备后续使用。</li>
</ol>
<h3 id="2-查询预处理和编码"><a href="#2-查询预处理和编码" class="headerlink" title="2. 查询预处理和编码"></a>2. 查询预处理和编码</h3><p>当你输入查询“机器学习的好处”时，ColBERT会立即对这个查询进行处理，这个过程是在线进行的。</p>
<ol>
<li><strong>分割查询</strong>：将查询分解成单词，比如“机器”、“学习”、“的”、“好处”。</li>
<li><strong>添加标记</strong>：在查询前面加上特殊标记，比如“[Q]”，以表明这是一个查询。</li>
<li><strong>填充和BERT编码</strong>：将查询填充到固定长度并输入BERT模型，生成每个单词的上下文向量。这些向量表示了查询中每个单词的意义和它们之间的关系。</li>
<li><strong>归一化和降维</strong>：对这些向量进行归一化处理，使它们与文档向量的格式一致。</li>
</ol>
<h3 id="3-延迟交互和相似度计算"><a href="#3-延迟交互和相似度计算" class="headerlink" title="3. 延迟交互和相似度计算"></a>3. 延迟交互和相似度计算</h3><p>接下来，ColBERT通过延迟交互和相似度计算来找到最相关的论文。</p>
<ol>
<li><strong>加载文档嵌入</strong>：从数据库中加载所有预先计算好的文档向量表示。</li>
<li><strong>最大相似度计算</strong>：对于查询中的每个单词向量，找到与文档中所有单词向量之间的最大相似度。这就像是找到最匹配的拼图块。</li>
<li><strong>相似度求和</strong>：将每个查询单词与文档单词的最大相似度值相加，得到一个总的相似度分数。这个分数表示了该文档与查询的相关性。</li>
</ol>
<h3 id="4-文档排序和检索"><a href="#4-文档排序和检索" class="headerlink" title="4. 文档排序和检索"></a>4. 文档排序和检索</h3><p>最后，根据相似度分数对文档进行排序，并返回得分最高的前k个文档。</p>
<ol>
<li><strong>文档排序</strong>：将所有候选文档按相似度得分进行排序，类似于将考试成绩从高到低排序。</li>
<li><strong>返回结果</strong>：返回得分最高的前k个文档，这些就是与你的查询最相关的论文。</li>
</ol>
<h3 id="生动形象的示例"><a href="#生动形象的示例" class="headerlink" title="生动形象的示例"></a>生动形象的示例</h3><p>想象你在图书馆里寻找“机器学习的好处”相关的书籍。图书馆员（ColBERT）事先已经对所有书籍进行了详细分类和标注（文档预处理和编码）。当你提出查询时，图书馆员快速浏览了每本书的内容（查询编码和相似度计算），找到了最相关的书籍并排列出来（文档排序和检索），最终将最相关的几本书递给你。这一切发生得非常迅速，因为图书馆员已经事先做好了大量准备工作。</p>
<p>通过这种方式，ColBERT既能保证高效处理大量数据，又能在查询时提供快速响应和高质量的结果。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.12832">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</a></p>
<h2 id="ColBERT-v2"><a href="#ColBERT-v2" class="headerlink" title="ColBERT v2"></a>ColBERT v2</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a></p>
<h3 id="ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction的主要改进和优化"><a href="#ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction的主要改进和优化" class="headerlink" title="ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction的主要改进和优化"></a><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>的主要改进和优化</h3><ol>
<li><strong>残差压缩机制</strong>：这是<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>的一个主要创新点。通过将每个嵌入向量编码为其最近质心的索引和量化残差，ColBERTv2大大减少了存储需求。这个改进在不牺牲模型质量的情况下显著降低了存储成本。</li>
<li><strong>降噪监督</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>引入了一种新的监督策略，包括交叉编码器蒸馏和困难负样本挖掘。这种方法选择具有挑战性的负样本，避免奖励假阳性或惩罚假阴性，从而提高了训练效果和模型质量。</li>
<li><strong>高效索引和检索</strong>：<ul>
<li><strong>质心选择</strong>：在索引阶段，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>通过质心选择优化了段落的表示。</li>
<li><strong>段落编码</strong>：通过调用BERT编码器并压缩输出嵌入，将每个嵌入分配到最近的质心并计算量化残差。</li>
<li><strong>索引倒置</strong>：为了支持快速最近邻搜索，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>将对应于每个质心的嵌入ID分组并保存倒排列表，这样在检索时可以快速找到相似的标记级嵌入。</li>
</ul>
</li>
<li><strong>检索过程优化</strong>：<ul>
<li><strong>候选生成</strong>：对于查询中的每个向量，找到最近的质心，并使用倒排列表识别接近这些质心的段落嵌入，解压缩它们，并计算它们与查询向量的余弦相似度。</li>
<li><strong>评分和排序</strong>：将分数按段落ID分组，并对同一段落的分数进行最大化减少，最终进行排序并返回结果。</li>
</ul>
</li>
</ol>
<h3 id="ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction-的全流程示例"><a href="#ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction-的全流程示例" class="headerlink" title="ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction 的全流程示例"></a><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a> 的全流程示例</h3><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p>假设我们有一个学术论文数据库，包含数百万篇论文。您是一名研究者，正在研究“机器学习的好处”，并希望找到最相关的论文。</p>
<h3 id="1-离线预处理和编码文档-1"><a href="#1-离线预处理和编码文档-1" class="headerlink" title="1. 离线预处理和编码文档"></a>1. 离线预处理和编码文档</h3><p><strong>步骤1：质心选择</strong></p>
<ol>
<li><strong>采样</strong>：从数百万篇论文中随机抽取一个样本，大小为语料库规模的平方根。</li>
<li><strong>聚类</strong>：对采样的论文片段进行k-means聚类，生成一组质心C。这些质心用于支持后续的残差编码和最近邻搜索。</li>
</ol>
<p><strong>步骤2：段落编码</strong></p>
<ol>
<li><strong>BERT编码</strong>：对于每篇论文，用BERT模型对其每个单词进行编码，生成一个嵌入向量。例如，一篇论文的片段“Machine learning is a method of data analysis”会被编码成多个嵌入向量。</li>
<li><strong>残差计算</strong>：对于每个嵌入向量，找到其最近的质心Ct，并计算量化残差r &#x3D; v - Ct。</li>
<li><strong>存储</strong>：将质心索引和量化残差一起存储到磁盘上。例如，段落中的一个嵌入向量可能被表示为最近质心的索引和一个小的量化残差。</li>
</ol>
<p><strong>步骤3：索引倒置</strong></p>
<ol>
<li><strong>倒排列表</strong>：将所有质心的索引分组，并保存倒排列表，以支持快速的最近邻搜索。这类似于将书籍按分类编号存放在书架上，以便快速找到。</li>
</ol>
<h3 id="2-在线查询处理和检索"><a href="#2-在线查询处理和检索" class="headerlink" title="2. 在线查询处理和检索"></a>2. 在线查询处理和检索</h3><p><strong>步骤1：查询编码</strong></p>
<ol>
<li><strong>BERT编码</strong>：当您输入查询“机器学习的好处”时，查询被分割成多个单词，并用BERT模型编码成嵌入向量。</li>
<li><strong>残差计算</strong>：对于每个查询嵌入向量，找到最近的质心Ct，并计算量化残差r &#x3D; v - Ct。</li>
</ol>
<p><strong>步骤2：候选生成</strong></p>
<ol>
<li><strong>最近质心查找</strong>：对于查询中的每个向量，找到最近的质心，并使用倒排列表识别接近这些质心的段落嵌入。</li>
<li><strong>解压缩</strong>：解压缩识别出的段落嵌入，并计算它们与查询向量的余弦相似度。</li>
<li><strong>评分和最大化减少</strong>：将分数按段落ID分组，并对同一段落的分数进行最大化减少。这类似于在拼图中找到最匹配的片段。</li>
</ol>
<p><strong>步骤3：排序和返回结果</strong></p>
<ol>
<li><strong>合并和排序</strong>：将所有分数相加，并按得分对段落进行排序。选择得分最高的前k个段落进行进一步评分。</li>
<li><strong>最终评分</strong>：加载每个段落的完整嵌入集，并对每个段落进行最终评分。</li>
<li><strong>返回结果</strong>：根据得分返回最相关的论文。这类似于从书架上找到最相关的几本书，并递给您。</li>
</ol>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>想象一下您在图书馆寻找“机器学习的好处”相关的书籍。图书馆员（ColBERTv2）事先已经对所有书籍进行了详细分类和标注，并将每本书的主要内容进行了数字化处理和压缩。当您提出查询时，图书馆员会迅速浏览每本书的数字化内容，找到最匹配的片段，将这些片段组合评分，最终将最相关的几本书递给您。这一切发生得非常迅速，因为图书馆员事先已经做好了大量准备工作。</p>
<h2 id="DPR"><a href="#DPR" class="headerlink" title="DPR"></a>DPR</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a></p>
<h3 id="示例场景"><a href="#示例场景" class="headerlink" title="示例场景"></a>示例场景</h3><p>假设用户向系统提出了一个问题：“什么是光合作用？”我们将通过以下步骤使用 DPR 检索相关信息：</p>
<h3 id="步骤-1-查询编码"><a href="#步骤-1-查询编码" class="headerlink" title="步骤 1: 查询编码"></a>步骤 1: 查询编码</h3><ul>
<li><strong>输入</strong>：用户的问题 “什么是光合作用？”</li>
<li><strong>处理</strong>：首先，这个问题被送入一个预训练的 Transformer 模型（如BERT）。这个模型将文本转换成一个高维向量（通常是 768 维或更多，依赖于模型架构）。</li>
<li><strong>输出</strong>：问题的密集向量表示。</li>
</ul>
<h3 id="步骤-2-文档库编码"><a href="#步骤-2-文档库编码" class="headerlink" title="步骤 2: 文档库编码"></a>步骤 2: 文档库编码</h3><ul>
<li><strong>预处理</strong>：在这一步骤之前，系统已经预先将可能的回答或信息源（如维基百科条目、教科书段落等）编码成向量，并存储在向量数据库中。</li>
<li><strong>数据库</strong>：包含了大量文档的向量表示，这些都是事先处理好的。</li>
</ul>
<h3 id="步骤-3-向量相似度计算"><a href="#步骤-3-向量相似度计算" class="headerlink" title="步骤 3: 向量相似度计算"></a>步骤 3: 向量相似度计算</h3><ul>
<li><strong>比较</strong>：系统现在将查询向量与文档库中的每一个文档向量进行比较。比较通常采用余弦相似度</li>
<li><strong>排名</strong>：基于相似度得分，所有文档按得分从高到低排序。</li>
</ul>
<h3 id="步骤-4-选择顶部文档"><a href="#步骤-4-选择顶部文档" class="headerlink" title="步骤 4: 选择顶部文档"></a>步骤 4: 选择顶部文档</h3><ul>
<li><strong>选择</strong>：系统通常选择相似度得分最高的前 N 个文档（例如，前 5 或 10 个），认为这些文档与查询最相关。</li>
<li><strong>输出</strong>：这些顶部文档的文本内容被送到生成模型，用于下一步的答案生成。</li>
</ul>
<h3 id="步骤-5-答案生成"><a href="#步骤-5-答案生成" class="headerlink" title="步骤 5: 答案生成"></a>步骤 5: 答案生成</h3><ul>
<li><strong>生成模型输入</strong>：选定的文档内容作为上下文输入到一个生成模型（如 GPT）中。</li>
<li><strong>生成答案</strong>：生成模型综合考虑这些文本信息，生成一个信息丰富且相关的答案。</li>
</ul>
<h3 id="步骤-6-输出最终答案"><a href="#步骤-6-输出最终答案" class="headerlink" title="步骤 6: 输出最终答案"></a>步骤 6: 输出最终答案</h3><ul>
<li><strong>用户接收</strong>：系统输出的答案展示给用户，例如：”光合作用是植物、藻类和某些细菌使用阳光将水和二氧化碳转化为氧气和葡萄糖的过程。”</li>
</ul>
<p>这个例子展示了 DPR 在 RAG 系统中如何精确地从大量信息中检索相关内容，并辅助生成模型以提供准确且有用的回答。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01585">Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications</a></p>
<blockquote>
<p>No code</p>
</blockquote>
<h2 id="RAFT"><a href="#RAFT" class="headerlink" title="RAFT"></a>RAFT</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.10131">RAFT: Adapting Language Model to Domain Specific RAG</a></p>
<p>本文提出了一种称为<strong>Retrieval Augmented Fine Tuning (RAFT)</strong> 的方法，旨在增强预训练语言模型在特定领域中的检索增强生成（RAG）能力，特别是在“开放书本”设置下。该方法通过将微调与RAG结合，以提升模型在领域特定问答任务中的表现。</p>
<p><strong>数据准备</strong>：RAFT方法为每个问题准备了一组文档，其中包含包含答案的“oracle”文档和不相关的干扰文档。对于包含正确文档的问题，生成链式思维（Chain-of-Thought, CoT）风格的答案，答案中直接引用了文档的相关片段，以减少生成过程中的幻觉问题（hallucination）。这种数据结构旨在训练模型更好地识别和利用相关信息。</p>
<p><strong>训练策略</strong>：在训练过程中，模型被微调以应对包含干扰文档的情境。模型被迫在这些干扰文档的干扰下，准确提取有用的信息并生成答案。此外，训练数据中部分问题仅包含干扰文档，以促使模型依赖已学得的领域知识进行回答。这种策略不仅强化了模型在特定领域内的知识，还提高了其在有噪声背景下作答的能力。</p>
<p><strong>微调与RAG的结合</strong>：RAFT通过微调模型，使其在开放书本的设置中更为有效地工作。在此过程中，模型学会了在处理特定领域的文档时，如何忽略无关信息并准确引用相关文档内容来生成答案。与传统的RAG方法不同，RAFT专注于特定领域的应用，进一步提高了模型的检索和生成能力。</p>
<p>实验结果表明，RAFT在多个数据集（如PubMed、HotpotQA、Gorilla API Bench）上显著优于其他基线模型，证明了其在领域特定问答任务中的强大潜力。本文为实现高效的领域特定问答任务提供了一种有效的训练策略，展示了微调与RAG相结合在提升模型性能方面的优势。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Lu, W., Zhang, J., Zhang, J. and Chen, Y., 2024. Large language model for table processing: A survey. arXiv preprint arXiv:2402.05121.</li>
<li>Fang, X., Xu, W., Tan, F.A., Zhang, J., Hu, Z., Qi, Y., Nickleach, S., Socolinsky, D., Sengamedu, S. and Faloutsos, C., 2024. Large Language Models on Tabular Data–A Survey. arXiv preprint arXiv:2402.17944.</li>
<li>Zhao, Y., Long, Y., Liu, H., Nan, L., Chen, L., Kamoi, R., Liu, Y., Tang, X., Zhang, R. and Cohan, A., 2023. Docmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data. arXiv preprint arXiv:2311.09805.</li>
<li>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023. Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. arXiv preprint arXiv:2312.09039.</li>
<li>Dong, X., Zhang, C., Ge, Y., Mao, Y., Gao, Y., Lin, J. and Lou, D., 2023. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306.</li>
<li>Sundar, A.S. and Heck, L., 2023. cTBLS: Augmenting large language models with conversational tables. arXiv preprint arXiv:2303.12024.</li>
<li>Gao, D., Wang, H., Li, Y., Sun, X., Qian, Y., Ding, B. and Zhou, J., 2023. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363.</li>
<li>Lin, W., Blloshmi, R., Byrne, B., de Gispert, A. and Iglesias, G., 2023, July. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 1557-1566).</li>
<li>Khattab, O. and Zaharia, M., 2020, July. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (pp. 39-48).</li>
<li>Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C. and Zaharia, M., 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488.</li>
<li>Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</li>
<li>Khanna, S. and Subedi, S., 2024. Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications. arXiv preprint arXiv:2405.01585.</li>
<li>Zhang, T., Patil, S.G., Jain, N., Shen, S., Zaharia, M., Stoica, I. and Gonzalez, J.E., 2024. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131.</li>
</ol>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/YuhangWuAI">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-for-Table"><span class="toc-number">1.</span> <span class="toc-text">LLM for Table</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-LLMs%E8%A1%A8%E6%A0%BC%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.</span> <span class="toc-text">1. LLMs表格处理方法分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95%E5%BD%92%E7%BA%B3"><span class="toc-number">1.2.</span> <span class="toc-text">具体方法归纳</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-on-Tabular-Data-Retriever"><span class="toc-number">2.</span> <span class="toc-text">LLM on Tabular Data (Retriever)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E6%99%AE%E9%81%8D%E8%83%BD%E5%8A%9B"><span class="toc-number">2.1.</span> <span class="toc-text">5.2 大型语言模型在问答任务中的普遍能力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E9%97%AE%E7%AD%94%EF%BC%88Numerical-QA%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">数值问答（Numerical QA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Text2SQL"><span class="toc-number">2.3.</span> <span class="toc-text">Text2SQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A4%A7%E5%B0%8F%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.4.</span> <span class="toc-text">模型大小对性能的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E8%BF%98%E6%98%AF%E4%B8%8D%E5%BE%AE%E8%B0%83%EF%BC%9F"><span class="toc-number">2.5.</span> <span class="toc-text">微调还是不微调？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E8%A1%A8%E6%A0%BC%E9%97%AE%E7%AD%94%E7%9A%84%E5%85%B3%E9%94%AE%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="toc-number">2.6.</span> <span class="toc-text">5.3 表格问答的关键组成部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-1-%E6%9F%A5%E8%AF%A2%E6%84%8F%E5%9B%BE%E6%B6%88%E6%AD%A7"><span class="toc-number">2.7.</span> <span class="toc-text">5.3.1 查询意图消歧</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E5%92%8C%E6%A3%80%E7%B4%A2%E6%96%B9%E6%B3%95%E5%BD%92%E7%BA%B3%E6%80%BB%E7%BB%93"><span class="toc-number">2.8.</span> <span class="toc-text">搜索和检索方法归纳总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A3%80%E7%B4%A2%E6%A8%A1%E5%9D%97%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87"><span class="toc-number">2.9.</span> <span class="toc-text">1. 检索模块性能提升</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A1%A8%E6%A0%BC%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95"><span class="toc-number">2.10.</span> <span class="toc-text">2. 表格采样方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%A1%A8%E6%A0%BC%E6%8E%92%E5%90%8D%E6%96%B9%E6%B3%95"><span class="toc-number">2.11.</span> <span class="toc-text">3. 表格排名方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%B8%89%E6%AD%A5%E6%A3%80%E7%B4%A2%E6%9E%B6%E6%9E%84"><span class="toc-number">2.12.</span> <span class="toc-text">4. 三步检索架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E7%A8%A0%E5%AF%86%E5%92%8C%E7%A8%80%E7%96%8F%E6%A3%80%E7%B4%A2%E5%99%A8%E7%BB%93%E5%90%88"><span class="toc-number">2.13.</span> <span class="toc-text">5. 稠密和稀疏检索器结合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%8E%B7%E5%8F%96%E9%A2%9D%E5%A4%96%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">2.14.</span> <span class="toc-text">6. 获取额外信息的方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%89%8B%E5%8A%A8%E5%92%8C%E9%9A%8F%E6%9C%BA%E7%A4%BA%E4%BE%8B%E9%80%89%E6%8B%A9"><span class="toc-number">2.15.</span> <span class="toc-text">7. 手动和随机示例选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LI-RAGE"><span class="toc-number">3.</span> <span class="toc-text">LI-RAGE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E7%AB%A0%E5%86%85%E5%AE%B9%E7%AE%80%E7%9F%AD%E6%80%BB%E7%BB%93"><span class="toc-number">3.1.</span> <span class="toc-text">文章内容简短总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E9%97%AE%E9%A2%98"><span class="toc-number">3.2.</span> <span class="toc-text">示例问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.3.</span> <span class="toc-text">表格数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">3.4.</span> <span class="toc-text">全流程示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%A1%A8%E6%A0%BC%E6%A3%80%E7%B4%A2%EF%BC%88Retriever%EF%BC%89"><span class="toc-number">3.4.1.</span> <span class="toc-text">1. 表格检索（Retriever）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%EF%BC%88Reader%EF%BC%89"><span class="toc-number">3.4.2.</span> <span class="toc-text">2. 答案生成（Reader）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BA%8C%E8%BF%9B%E5%88%B6%E7%9B%B8%E5%85%B3%E6%80%A7%E6%A0%87%E8%AE%B0%EF%BC%88Binary-Relevance-Token%EF%BC%89"><span class="toc-number">3.4.3.</span> <span class="toc-text">3. 二进制相关性标记（Binary Relevance Token）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E8%BF%87%E6%BB%A4%E4%B8%8E%E7%A1%AE%E5%AE%9A%E6%9C%80%E7%BB%88%E7%AD%94%E6%A1%88"><span class="toc-number">3.4.4.</span> <span class="toc-text">4. 过滤与确定最终答案</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TAP4LLM"><span class="toc-number">4.</span> <span class="toc-text">TAP4LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TAP4LLM%E6%96%B9%E6%B3%95%E8%AF%A6%E7%BB%86%E6%80%BB%E7%BB%93"><span class="toc-number">4.1.</span> <span class="toc-text">TAP4LLM方法详细总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">4.2.</span> <span class="toc-text">大型语言模型在表格数据中的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF"><span class="toc-number">4.3.</span> <span class="toc-text">表格增强技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TAP4LLM%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">4.4.</span> <span class="toc-text">TAP4LLM的核心组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">4.5.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%90%E5%88%B6"><span class="toc-number">4.6.</span> <span class="toc-text">限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%9A%E4%BD%BF%E7%94%A8TAP4LLM%E8%BF%9B%E8%A1%8C%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">4.7.</span> <span class="toc-text">示例：使用TAP4LLM进行表格数据分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%A1%A8%E6%A0%BC%E9%87%87%E6%A0%B7"><span class="toc-number">4.8.</span> <span class="toc-text">1. 表格采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A1%A8%E6%A0%BC%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.9.</span> <span class="toc-text">2. 表格增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%A1%A8%E6%A0%BC%E6%89%93%E5%8C%85%E4%B8%8E%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">4.10.</span> <span class="toc-text">3. 表格打包与序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%9C%80%E7%BB%88%E5%88%86%E6%9E%90%E7%BB%93%E6%9E%9C"><span class="toc-number">4.11.</span> <span class="toc-text">4. 最终分析结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90%EF%BC%9A%E4%BD%BF%E7%94%A8TAP4LLM%E5%9B%9E%E7%AD%94%E9%87%91%E8%9E%8D%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98"><span class="toc-number">4.12.</span> <span class="toc-text">例子：使用TAP4LLM回答金融数据问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E8%A1%A8%E6%A0%BC%E5%92%8C%E9%97%AE%E9%A2%98"><span class="toc-number">4.13.</span> <span class="toc-text">1. 输入表格和问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%A1%A8%E6%A0%BC%E9%87%87%E6%A0%B7"><span class="toc-number">4.14.</span> <span class="toc-text">2. 表格采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%A1%A8%E6%A0%BC%E5%A2%9E%E5%BC%BA"><span class="toc-number">4.15.</span> <span class="toc-text">3. 表格增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%A1%A8%E6%A0%BC%E6%89%93%E5%8C%85%E4%B8%8E%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">4.16.</span> <span class="toc-text">4. 表格打包与序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E7%BB%93%E6%9E%9C%E7%94%9F%E6%88%90"><span class="toc-number">4.17.</span> <span class="toc-text">5. 结果生成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ColBERT"><span class="toc-number">5.</span> <span class="toc-text">ColBERT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">5.1.</span> <span class="toc-text">创新点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">5.2.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">5.3.</span> <span class="toc-text">解决的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ColBERT-%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B%EF%BC%9A%E8%AF%A6%E7%BB%86%E5%92%8C%E7%94%9F%E5%8A%A8%E7%9A%84%E8%AE%B2%E8%A7%A3"><span class="toc-number">5.4.</span> <span class="toc-text">ColBERT 使用流程示例：详细和生动的讲解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">5.5.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%A6%BB%E7%BA%BF%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%96%E7%A0%81%E6%96%87%E6%A1%A3"><span class="toc-number">5.6.</span> <span class="toc-text">1. 离线预处理和编码文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%9F%A5%E8%AF%A2%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%96%E7%A0%81"><span class="toc-number">5.7.</span> <span class="toc-text">2. 查询预处理和编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%BB%B6%E8%BF%9F%E4%BA%A4%E4%BA%92%E5%92%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">5.8.</span> <span class="toc-text">3. 延迟交互和相似度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%96%87%E6%A1%A3%E6%8E%92%E5%BA%8F%E5%92%8C%E6%A3%80%E7%B4%A2"><span class="toc-number">5.9.</span> <span class="toc-text">4. 文档排序和检索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E5%8A%A8%E5%BD%A2%E8%B1%A1%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.10.</span> <span class="toc-text">生动形象的示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ColBERT-v2"><span class="toc-number">6.</span> <span class="toc-text">ColBERT v2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction%E7%9A%84%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E5%92%8C%E4%BC%98%E5%8C%96"><span class="toc-number">6.1.</span> <span class="toc-text">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction的主要改进和优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction-%E7%9A%84%E5%85%A8%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">6.2.</span> <span class="toc-text">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction 的全流程示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF-1"><span class="toc-number">6.3.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%A6%BB%E7%BA%BF%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%96%E7%A0%81%E6%96%87%E6%A1%A3-1"><span class="toc-number">6.4.</span> <span class="toc-text">1. 离线预处理和编码文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%9C%A8%E7%BA%BF%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E5%92%8C%E6%A3%80%E7%B4%A2"><span class="toc-number">6.5.</span> <span class="toc-text">2. 在线查询处理和检索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">6.6.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DPR"><span class="toc-number">7.</span> <span class="toc-text">DPR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E5%9C%BA%E6%99%AF"><span class="toc-number">7.1.</span> <span class="toc-text">示例场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-1-%E6%9F%A5%E8%AF%A2%E7%BC%96%E7%A0%81"><span class="toc-number">7.2.</span> <span class="toc-text">步骤 1: 查询编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-2-%E6%96%87%E6%A1%A3%E5%BA%93%E7%BC%96%E7%A0%81"><span class="toc-number">7.3.</span> <span class="toc-text">步骤 2: 文档库编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-3-%E5%90%91%E9%87%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">7.4.</span> <span class="toc-text">步骤 3: 向量相似度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-4-%E9%80%89%E6%8B%A9%E9%A1%B6%E9%83%A8%E6%96%87%E6%A1%A3"><span class="toc-number">7.5.</span> <span class="toc-text">步骤 4: 选择顶部文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-5-%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90"><span class="toc-number">7.6.</span> <span class="toc-text">步骤 5: 答案生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4-6-%E8%BE%93%E5%87%BA%E6%9C%80%E7%BB%88%E7%AD%94%E6%A1%88"><span class="toc-number">7.7.</span> <span class="toc-text">步骤 6: 输出最终答案</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAFT"><span class="toc-number">8.</span> <span class="toc-text">RAFT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">9.</span> <span class="toc-text">Reference</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&text=LLM for Table data enhancement (CN)"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&is_video=false&description=LLM for Table data enhancement (CN)"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=LLM for Table data enhancement (CN)&body=Check out this article: http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&title=LLM for Table data enhancement (CN)"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&name=LLM for Table data enhancement (CN)&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://yuhangwuai.github.io/2024/08/13/LLM-for-Table-data-enhancement-CN/&t=LLM for Table data enhancement (CN)"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    YuhangWu
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/YuhangWuAI">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
